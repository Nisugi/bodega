=begin

  this script uses the new playershop system by Naos to parse in-game shop directories
  and generate JSON files that can be consumed by external systems.

  This script also exposes the Bodega module that other scripts may call.

  ;bodega --help is your friend

  Author: Ondreian
  Requirements: Ruby >= 2.3
  version: 0.4.2
  tags: playershops

  changelog:
    v0.4.2 - Remove unnecessary item cache for simplified smart caching
             - Only shop cache needed for browse data comparison
             - Load cached item data from existing JSON files on demand
             - Significantly reduced cache file sizes
    v0.4.1 - Fix smart caching logic for proper cache efficiency
             - Handle "unknown" prices from bootstrap correctly
             - Only re-inspect items with actual price changes
             - Improved browse data comparison logic
    v0.4 - Add smart caching system for 90%+ performance improvement
           - Smart item inspection (only scan new/changed items)
           - Cache bootstrap from existing JSON files
           - Selective upload (only upload parsed files)
           - Enhanced error handling and cache validation
    v0.3 - Update for Lich 5.11 compatibility
    v0.2 - Update for Ruby v3 compatibility

=end
require "ostruct"
require "json"
require "net/http"
require "fileutils"
require "pp"
require "pathname"
require "set"

unless Gem::Version.new(RUBY_VERSION) >= Gem::Version.new("2.3")
  fail "your ruby@#{RUBY_VERSION} is too old"
end
##
## check if a String is an int
##
class ::String
  def is_i?
    !!(self =~ /\A[-+]?[0-9]+\z/)
  end
end

##
## polyfill for working with MatchData
##
class ::MatchData
  def to_struct
    OpenStruct.new to_h
  end

  def to_h
    Hash[self.names.map(&:to_sym).zip(self.captures.map(&:strip).map do |capture|
      if capture.is_i? then capture.to_i else capture end
    end)]
  end
end

module Bodega
  ##
  ## contextual logging
  ##
  module Log
    def self.out(msg, label: :debug)
      if msg.is_a?(Exception)
        msg = %{
          #{msg.message}
          #{msg.backtrace.join("\n")}
        }
      end

      msg = _view(msg, label)

      if Opts.headless
        $stdout.write(msg + "\n")
      else
        _respond Preset.as(:debug, msg)
      end
    end

    def self._view(msg, label)
      label = [Script.current.name, label].flatten.compact.join(".")
      safe = msg.inspect
      safe = safe.gsub("<", "(").gsub(">", ")") if safe.include?("<") and safe.include?(">")
      "[#{label}] #{safe}"
    end

    def self.pp(msg, label = :debug)
      respond _view(msg, label)
    end

    def self.dump(*args)
      pp(*args)
    end

    module Preset
      def self.as(kind, body)
        %[<preset id="#{kind}">#{body}</preset>]
      end
    end
  end

  ##
  ## minimal options parser
  ##
  module Opts
    FLAG_PREFIX = "--"

    def self.parse_command(h, c)
      # Handle --parse as alias for --parser
      if c == "parse"
        h[:parser] = true
      # Handle smart caching aliases
      elsif c == "smart"
        h[:smart] = true
      elsif c == "force-full"
        h[:"force-full"] = true
      else
        h[c.to_sym] = true
      end
    end

    def self.parse_flag(h, f)
      (name, val) = f[2..-1].split("=")
      if val.nil?
        h[name.to_sym] = true
      else
        val = val.split(",")

        h[name.to_sym] = val.size == 1 ? val.first : val
      end
    end

    def self.parse(args = Script.current.vars[1..-1])
      return @opts ||= _parse(args) if @script.eql?(Script.current)
      @script = Script.current
      return @opts = _parse(args) if @script.eql?(Script.current)
    end

    def self._parse(args)
      OpenStruct.new(**args.to_a.reduce(Hash.new) do |opts, v|
        if v.start_with?(FLAG_PREFIX)
          Opts.parse_flag(opts, v)
        else
          Opts.parse_command(opts, v)
        end
        opts
      end)
    end

    def self.method_missing(method, *args)
      parse.send(method, *args)
    end
  end
end

module Bodega
  module Messages
    TOWNS    = %r[Valid options include: (?<towns>.*?)\.]
    COMMAND  = %r[You can use the (?<command>.*?) command to browse the inventory of a particular shop.]
    NEW_ROOM = %r[^(?<room_title>.*)\s\((?<branch>\w+)\)$]
    SIGN     = %r[^Written on]
    ITEM     = %r[^(?<item_id>\d+)\)]
    # Pattern to capture full browse item data: ID, name, and price
    ITEM_BROWSE = %r[^\s*(?<item_id>\d+)\)\s+(?<name>.*?)\s+for\s+(?<price>[\d,]+)\s+silver]
  end
end

module Bodega
  class Collector
    MAX_TRIES = Opts.to_h.fetch("max-tries", 3)

    attr_reader :start, :close, :command

    def initialize(start:, close:, command:)
      @start   = start
      @close   = close
      @command = command
    end

    def blow_up(ttl)
      fail StandardError, "Collector(start: #{@start}, close: #{@close}) failed to complete in #{ttl} seconds"
    end

    def compare(line, pattern)
      return line.include?(pattern) if pattern.is_a?(String)
      return pattern.match(line)    if pattern.is_a?(Regexp)
      fail "Unable to compare #{pattern.class} <=> #{line.class}"
    end

    def run(seconds = 5, tries: 0)
      Log.out(@command, label: :retry) if tries > 0
      fput @command
      result = []
      ttl    = Time.now + seconds
      while (Time.now < ttl)
        line = get?
        if line.nil?
          sleep 0.1
          next
        end
        result.push(line) if compare(line, @start) or not result.empty?
        return result     if compare(line, @close) and not result.empty?
      end

      return run(seconds, tries: tries + 1) if Time.now > ttl and tries < Collector::MAX_TRIES
      return blow_up(seconds) if Time.now > ttl and tries > Collector::MAX_TRIES
    end
  end
end

module Bodega
  class Extractor
    BLACKLIST = Regexp.union(
      %r[^There is nothing there to read\.$],
      %r[^You carefully inspect],
      %r[^You get no sense of whether or not (.*?) may be further lightened.],
      %r[there is no recorded information on that item],
      %r[^You determine that you could not wear the shard\.],
      %r[^You see nothing unusual\.$],
      %r[^It imparts no bonus more than usual\.$],
      %r[^It is difficult to see the (.*?) clearly from this distance\.]
    )

    ENHANCIVE = {
      boost: %r[^It provides a boost of (?<boost>\d+) to (?<ability>.*?)\.],
      level_req: %r[^This enhancement may not be used by adventurers who have not trained (?<level>\d+) times.],
    }

    BOOLS = {
      max_deep: %r[pockets could not possibly get any deeper],
      max_light: %r[^You can tell that the (?:.*?) is as light as it can get],
      purpose: %r[appears to serve some purpose],
      deepenable: %r[you might be able to have a talented merchant deepen its pockets for you],
      lightenable: %r[You might be able to have a talented merchant lighten (.*?) for you.],
      persists: %r[^It will persist after its last charge is depleted|^It will persist after its last enhancive charge],
      crumbly: %r[but crumble after its last enhancive charge is depleted|^It will crumble into dust after its last charge is depleted\.$|^It will disintegrate after its last charge is depleted\.$],
      small: %r[^It is a small item, under a pound],
      imbeddable: %r[^It is a magical item which could be imbedded with a spell],
      not_wearable: %r[^You determine that you could not wear],
      holy: %r[^It is a holy item\.],
    }

    PROPS = {
      skill: %r[requires skill in (?<skill>.*?) to use effectively\.],
      enchant: %r[^It imparts a bonus of \+(?<enchant>\d+) more than usual\.],
      weight: %r[^It appears to weigh about (?<weight>\d+) pounds.],
      # flare: %r[^It has been infused with the power of an? (?<flare>.*?)\.],
      material: %r[^It looks like this item has been mainly crafted out of (?<material>.*?)\.],
      cost: %r[will cost (?<cost>\d+) coins\.$],
      worn: %r[You determine that you could wear the (?:.*?)(?:, slinging it| around| on| over) (?<worn>.*?)\.],
      activator: %r[^It could be activated by (?<activator>\w+) it\.$],
      spell: %r[^It is currently imbedded with the (?<spell>.*?) spell.],
      charges: %r[The (?:\w+) looks to have (?<charges>.*?) charges remaining\.$|It has (?<charges>.*?) charges remaining.],
      shield_size: %r[Your careful inspection of an ornate villswood shield allows you to conclude that it is a (?<size>\w+) shield that],
      armor_type: %r[ allows you to conclude that it is (?<armor_type>.*?)\.],
      flare: %r[It has been infused with (?<flare>.*?)\.$],
    }

    def self.of(details)
      # todo: implement detail extractor
      return new(details).to_h
    end

    attr_reader :props

    def initialize(details)
      @props = { raw: [], tags: [] }
      _extract(details)
    end

    def maybe_raw(line, *others)
      return if BLACKLIST.match(line)
      return if others.any? { |identity| identity }
      @props[:raw] << line
    end

    def _extract(details)
      details.each do |line|
        maybe_raw(line,
                  _props(line),
                  _bools(line),
                  _enhancive(line.strip))
      end
    end

    def _bools(line)
      BOOLS.select do |prop, pattern|
        line.match(pattern) and @props[:tags].push(prop)
      end.size > 0
    end

    def _enhancive(line)
      if (boost = line.match(ENHANCIVE[:boost]))
        enhancives = @props[:enhancives] ||= []
        enhancives.push(boost.to_h)
        return true
      end

      if (level_req = line.match(ENHANCIVE[:level_req]))
        enhancives.last.merge!(level_req.to_h)
        return true
      end

      return false
    end

    def _props(line)
      PROPS.select do |_prop, pattern|
        if (result = line.match(pattern))
          @props.merge!(result.to_h)
        else
          false
        end
      end.size > 0
    end

    def to_h
      @props
    end
  end
end

module Bodega
  # Smart caching module for item details
  module Cache
    def self.cache_dir
      Bodega::Assets::LOCAL_FS_ROOT
    end


    def self.shop_cache_file
      cache_dir + "shop_cache.json"
    end


    def self.load_shop_cache
      return {} unless File.exist?(shop_cache_file)
      begin
        cache = JSON.parse(File.read(shop_cache_file))
        validate_shop_cache(cache)
        cache
      rescue JSON::ParserError => e
        Log.out("Shop cache corrupted, starting fresh: #{e.message}", label: :cache)
        {}
      rescue => e
        Log.out("Shop cache invalid, starting fresh: #{e.message}", label: :cache)
        {}
      end
    end

    def self.save_shop_cache(cache)
      FileUtils.mkdir_p(cache_dir)
      File.write(shop_cache_file, JSON.pretty_generate(cache))
    end



    def self.get_cache_max_age
      (Opts.send(:"cache-max-age") || 7).to_i
    end

    def self.cache_empty?
      !File.exist?(shop_cache_file)
    end

    def self.find_existing_json_files
      Dir.glob(cache_dir + "*.json").reject do |f|
        f.include?("shop_cache.json") ||
        f.include?("upload_tracking.json") ||
        f.include?("manifest.json")
      end
    end

    def self.bootstrap_cache_from_json
      existing_files = find_existing_json_files
      return if existing_files.empty?

      Log.out("Bootstrapping cache from #{existing_files.size} existing JSON files...", label: :cache)

      shop_cache = {}
      bootstrapped_items = 0

      existing_files.each do |json_file|
        begin
          town_data = JSON.parse(File.read(json_file))
          town_name = File.basename(json_file, '.json')

          next unless town_data['shops']

          shop_cache[town_name] ||= {}

          town_data['shops'].each do |shop|
            shop_id = shop['id'].to_s
            browse_data = {}

            next unless shop['inv']

            shop['inv'].each do |room|
              next unless room['items']

              room['items'].each do |item|
                item_id = item['id'].to_s

                # Extract browse data from existing JSON for comparison
                browse_data[item_id] = {
                  'name' => item['name'],
                  'price' => item.dig('details', 'cost') || 'unknown'
                }

                bootstrapped_items += 1
              end
            end

            # Populate shop cache with browse data for comparison
            shop_cache[town_name][shop_id] = {
              'last_scan' => town_data['created_at'] || Time.now.strftime("%Y-%m-%dT%H:%M:%SZ"),
              'browse_data' => browse_data
            }
          end

        rescue => e
          Log.out("Failed to bootstrap from #{json_file}: #{e.message}", label: :cache)
        end
      end

      if bootstrapped_items > 0
        save_shop_cache(shop_cache)
        Log.out("Cache bootstrapped: #{bootstrapped_items} items from #{existing_files.size} files", label: :cache)
      else
        Log.out("No valid data found for cache bootstrap", label: :cache)
      end
    end


    def self.validate_shop_cache(cache)
      return unless cache.is_a?(Hash)

      cache.each do |town, shops|
        next unless shops.is_a?(Hash)
        shops.each do |shop_id, shop_data|
          unless shop_data.is_a?(Hash) && shop_data['last_scan'] &&
                 (shop_data['browse_data'].is_a?(Hash) || shop_data['item_ids'].is_a?(Array))
            raise "Invalid shop cache structure for #{town}/#{shop_id}"
          end
        end
      end
    end
  end

  module Assets
    $lich_dir = "/tmp" unless defined? $lich_dir

    LOCAL_FS_ROOT = Pathname.new($lich_dir) + (Opts.local_dir || "bodega")
    URL           = Opts.remote || "https://bodega.surge.sh"

    begin
      FileUtils.mkdir_p(LOCAL_FS_ROOT)
    rescue => exception
      Log.out(exception.message)
      exit
    end

    def self.local_path(file)
      LOCAL_FS_ROOT + file
    end

    def self.remote_path(file)
      [URL, file].join("/")
    end

    def self.checksum(file)
      require "digest"
      file = local_path(file)
      if File.exist?(file)
        Digest::SHA256.file(file).hexdigest
      else
        false
      end
    end

    def self.glob(pattern)
      Dir[LOCAL_FS_ROOT + pattern]
    end

    def self.read_local_json(file)
      Utils.read_json local_path(file)
    end

    def self.write_local_json(data, file)
      Utils.write_json(data,
                       local_path(file + ".json"))
    end

    def self.get_remote(file)
      url = remote_path(file)
      uri = URI(url)
      Net::HTTP.get(uri)
    end

    def self.is_stale?(remote)
      remote = OpenStruct.new(remote)
      not Assets.checksum(remote.base_name).eql?(remote.checksum)
    end

    def self.cached_files()
      glob("*.json").reject do |f| f.include?("manifest.json") end
    end

    def self.stream_download(remote)
      require("open-uri")
      remote = OpenStruct.new(remote)
      path = local_path(remote.base_name)
      Log.out("updating".rjust(10) + " ... #{remote.base_name.gsub(".json", "").rjust(20)} >> #{remote.size}", label: :download)
      # rubocop:disable Security/Open
      IO.copy_stream(open(remote.url), path)
      # rubocop:enable Security/Open
    end
  end
end

module Bodega
  module Utils
    def self.parse_json(str)
      begin
        JSON.parse(str, symbolize_names: true)
      rescue => exception
        Script.log(exception)
        Script.log(exception.backtrace)
        return {}
      end
    end

    def self.read_json(file)
      parse_json File.read(file)
    end

    def self.write_json(data, file)
      Log.out("... writing #{file}", label: :filesystem)
      File.open(file, 'w') do |f|
        data = JSON.pretty_generate(data) unless data.is_a?(String)
        f.write(data)
      end
    end

    def self.fmt_time(diff)
      return "#{(diff * 1_000.0).to_i}ms" if diff < 1
      (h, m, s) = (diff / 60).as_time.split(":").map(&:to_i)

      h = h % 24
      d = h / 24

      fmted = []
      fmted << d.to_s.rjust(2, "0") + "d" if d > 0
      fmted << h.to_s.rjust(2, "0") + "h" if h > 0
      fmted << m.to_s.rjust(2, "0") + "m" if m > 0
      fmted << s.to_s.rjust(2, "0") + "s" if s > 0
      return fmted.join(" ")
    end

    def self.benchmark(**args)
      template = args.fetch(:template, "{{run_time}}")
      label    = args.fetch(:label, :benchmark)
      start    = Time.now
      result   = yield
      run_time = Utils.fmt_time(Time.now - start)
      Log.out(template.gsub("{{run_time}}", run_time), label: label)
      return result
    end

    def self.safe_string(t)
      t.to_s.downcase
       .gsub("ta'", "ta_")
       .gsub(/'|,/, "")
       .gsub(/-|\s/, "_")
    end

    def self.pp(o)
      begin
        _respond PP.pp(o, "")
      rescue => _ex
        respond o.inspect
      end
    end
  end
end

module Bodega
  module Parser
    def self.fetch_towns()
      case (result = dothistimeout("shop direc", 5, Messages::TOWNS))
      when Messages::TOWNS
        result.match(Messages::TOWNS)[:towns].gsub(" and ", ", ").split(", ")
      else
        fail "unknown outcome for parsing available towns"
      end
    end

    def self.towns()
      @towns ||= fetch_towns
    end

    def self.shops(town)
      coll = Collector.new(
        command: %[shop direc #{town}],
        start: %r[~*~ (?<title>.*?) Shops ~*~],
        close: %[You can use the SHOP BROWSE]
      )

      result = coll.run()

      next_command = result.last.match(Messages::COMMAND)[:command]

      by_shop_number = result.slice(1..-2)
                             .map(&:strip).map do |row| row.split(/\s{2,}/) end # split columns
                             .flatten.map do |col| col.split(") ") end          # split number/shop name

      if Opts.shop
        by_shop_number = Hash[by_shop_number.select do |_id, title| title.downcase.include?(Opts.shop.downcase) end]
      end

      Parser.scan_shops(town, Hash[by_shop_number], next_command)
    end

    def self.scan_shops(town, shops, cmd_template)
      max_shop_depth = (Opts["max-shop-depth"] || 10_000).to_i
      shops = shops.take(max_shop_depth)
      shops.map.with_index do |k_v, idx|
        (id, name) = k_v
        right_col = "scanning [#{idx + 1}/#{shops.size}]".rjust(30)
        Log.out(right_col + " ... Shop(id: #{id}, name: #{name})",
                label: Utils.safe_string(town))
        begin
          Parser.scan_inv(town, id, name,
                          cmd_template.gsub("{SHOP#}", id))
        rescue => exception
          Log.out(exception)
          nil
        end
      end.compact # prune errored shops
    end

    def self.scan_inv(town, id, _name, cmd)
      coll = Collector.new(
        start: %[is located in],
        close: %[You can use the SHOP INSPECT {STOCK#}],
        command: cmd
      )

      (preamble, *inv) = coll.run()

      inventory_data = if Opts.smart && !Opts.send(:"force-full")
        smart_parse_inv(town, id, inv.map(&:strip))
      else
        parse_inv(inv.map(&:strip))
      end

      { preamble: preamble,
        town: town,
        id: id,
        inv: inventory_data }
    end

    def self.add_room(acc, row)
      acc.push(
        OpenStruct.new(
          row.match(Messages::NEW_ROOM).to_h.merge({ items: [] })
        )
      )
    end

    def self.add_sign(acc, row)
      acc.last.sign ||= []
      acc.last.sign.push(row)
    end

    def self.add_item(acc, row)
      # Try to capture full browse data first, fallback to ID only
      if browse_match = row.match(Messages::ITEM_BROWSE)
        item_data = browse_match.to_h
        acc.last.items.push({
          'id' => item_data[:item_id],
          'name' => item_data[:name].strip,
          'price' => item_data[:price]
        })
      elsif id_match = row.match(Messages::ITEM)
        # Fallback for items without price info
        acc.last.items.push({
          'id' => id_match[:item_id],
          'name' => nil,
          'price' => nil
        })
      end
    end

    def self.parse_inv(inv)
      inv.reduce([]) do |acc, row|
        # we are at the terminal line for this shop
        break acc if row.include?("SHOP INSPECT")
        Parser.add_room(acc, row) if row.match(Messages::NEW_ROOM)
        Parser.add_sign(acc, row) if row.match(Messages::SIGN) or not acc.last.sign.nil?
        Parser.add_item(acc, row) if row.match(Messages::ITEM) and acc.last.sign.nil?
        acc
      end
      .map do |room|
        max_item_depth = (Opts["max-item-depth"] || 100).to_i
        room.items = room.items.take(max_item_depth).map do |item_data|
          item_id = item_data.is_a?(Hash) ? item_data['id'] : item_data
          Parser.scan_item(item_id)
        end
        room
      end
      .map(&:to_h)
    end

    def self.smart_parse_inv(town, shop_id, inv)
      # Check if cache needs bootstrapping from existing JSON files
      if Cache.cache_empty?
        Cache.bootstrap_cache_from_json
      end

      # Load shop cache
      shop_cache = Cache.load_shop_cache

      # Parse current shop inventory (collect browse data)
      current_rooms = inv.reduce([]) do |acc, row|
        break acc if row.include?("SHOP INSPECT")
        Parser.add_room(acc, row) if row.match(Messages::NEW_ROOM)
        Parser.add_sign(acc, row) if row.match(Messages::SIGN) or not acc.last.sign.nil?
        Parser.add_item(acc, row) if row.match(Messages::ITEM) and acc.last.sign.nil?
        acc
      end

      # Get current browse data for comparison
      current_browse_data = {}
      current_rooms.each do |room|
        (room.items || []).each do |item_data|
          next unless item_data.is_a?(Hash)
          current_browse_data[item_data['id']] = {
            'name' => item_data['name'],
            'price' => item_data['price']
          }
        end
      end

      # Get cached browse data for this shop
      shop_cache[town] ||= {}
      cached_shop_data = shop_cache[town][shop_id] || {}
      cached_browse_data = cached_shop_data['browse_data'] || {}

      # Compare browse data to determine what changed
      current_ids = Set.new(current_browse_data.keys)
      cached_ids = Set.new(cached_browse_data.keys)

      new_items = current_ids - cached_ids
      removed_items = cached_ids - current_ids
      existing_items = current_ids & cached_ids

      # Check for price/name changes in existing items
      # Items with "unknown" price (from bootstrap) don't need re-inspection, just price update
      changed_items = existing_items.select do |item_id|
        current_item = current_browse_data[item_id]
        cached_item = cached_browse_data[item_id]

        # Name change always requires re-inspection
        return true if current_item['name'] != cached_item['name']

        # Price change: only re-inspect if cached price was not "unknown"
        if current_item['price'] != cached_item['price']
          cached_item['price'] != 'unknown'
        else
          false
        end
      end

      items_to_inspect = new_items + changed_items
      items_from_cache = existing_items - changed_items

      # Calculate cache efficiency
      total_items = current_ids.size
      cache_hits = items_from_cache.size
      cache_efficiency = total_items > 0 ? (cache_hits.to_f / total_items * 100).round(1) : 0

      Log.out("Shop #{shop_id}: #{new_items.size} new, #{changed_items.size} changed, #{items_from_cache.size} cached (#{cache_efficiency}% efficiency), #{removed_items.size} removed", label: :cache)

      # Inspect only items that need it
      newly_inspected = {}
      if items_to_inspect.any?
        max_item_depth = (Opts["max-item-depth"] || 100).to_i
        items_to_inspect.take(max_item_depth).each do |item_id|
          begin
            item_data = Parser.scan_item(item_id)
            newly_inspected[item_id] = item_data
          rescue => e
            Log.out("Failed to inspect item #{item_id}: #{e.message}", label: :cache)
          end
        end
      end

      # Update shop cache with current browse data
      shop_cache[town][shop_id] = {
        'last_scan' => Time.now.strftime("%Y-%m-%dT%H:%M:%SZ"),
        'browse_data' => current_browse_data
      }

      # Build rooms with newly inspected data only
      # For cached items, we'll need to load from existing JSON on demand
      current_rooms.map do |room|
        room.items = (room.items || []).map do |item_data|
          item_id = item_data.is_a?(Hash) ? item_data['id'] : item_data

          if newly_inspected[item_id]
            newly_inspected[item_id]
          elsif items_from_cache.include?(item_id)
            # For cached items, find existing data from town JSON
            find_existing_item_data(town, item_id)
          else
            # Fallback: inspect item if not handled above
            Log.out("Cache miss for item #{item_id}, inspecting", label: :cache)
            Parser.scan_item(item_id)
          end
        end
        room
      end.map(&:to_h).tap do
        # Save updated shop cache
        Cache.save_shop_cache(shop_cache)
      end
    end

    def self.find_existing_item_data(town, item_id)
      # Load existing town JSON to find cached item data
      town_file = Cache.cache_dir + "#{town}.json"
      return nil unless File.exist?(town_file)

      begin
        town_data = JSON.parse(File.read(town_file))
        town_data['shops']&.each do |shop|
          shop['inv']&.each do |room|
            room['items']&.each do |item|
              return item if item['id'].to_s == item_id.to_s
            end
          end
        end
      rescue => e
        Log.out("Failed to load cached item data from #{town_file}: #{e.message}", label: :cache)
      end

      nil
    end

    def self.scan_item(id)
      coll = Collector.new(
        command: %[shop inspect #{id}],
        start: %[You request a thorough inspection of],
        close: %[You can use SHOP PURCHASE #{id} to purchase]
      )

      details = coll.run()

      { id: id,
        name: details.first.gsub("You request a thorough inspection of ", "").gsub(%r[\sfrom [A-Z].*?\.$], ""),
        details: Extractor.of(details.slice(1..-2)) }
    end

    def self.towns_to_search()
      if Opts.town.nil? then
        towns
      else
        towns.select do |town| town.downcase.include?(Opts.town.downcase) end
      end
    end

    def self.all()
      towns_to_search.map do |town|
        { town: town,
          shops: Parser.shops(town) }
      end
    end

    def self.to_json()
      @created_files = []
      Utils.benchmark(template: "scanned shops in {{run_time}}") do
        towns_to_search.each do |town|
          start = Time.now
          shops = Parser.shops(town)

          next if shops.empty?

          data = {
            created_at: Time.now.utc,
            run_time: Utils.fmt_time(Time.now - start),
            town: town,
            shops: shops,
          }

          filename = save_json(name: town, data: data)
          @created_files << filename if filename
        end
      end

      # Smart caching cleanup is handled per-shop during parsing

      @created_files
    end

    def self.created_files
      @created_files || []
    end

    def self.save_json(name:, data:)
      return Utils.pp(data) if Opts["dry-run"]
      filename = Utils.safe_string(Opts.to_h.fetch(:shop, name))
      Assets.write_local_json(data, filename)
      return "#{filename}.json"
    end

    def self.manifest(url_root: Opts.to_h.fetch(:url, Assets::URL), _file: nil)
      assets = Assets.cached_files()
      fail "no assets found" if assets.empty?
      assets = assets.map do |asset|
        abs_file_name = Assets.local_path(asset)
        base_name     = File.basename(asset)
        data          = Utils.read_json(abs_file_name)
        { url: url_root + "/" + base_name,
          base_name: base_name,
          size: '%.2fmb' % (File.size(abs_file_name).to_f / (2**20)), # mbs
          run_time: data.fetch(:run_time, nil),
          checksum: Assets.checksum(asset),
          updated_at: data.fetch(:created_at, nil) }
      end

      manifest = { created_at: Time.now.utc, assets: assets }
      Utils.pp(manifest)
      return if Opts["dry-run"]
      Assets.write_local_json(manifest, "manifest")
    end
  end
end

module Bodega
  class Index
    class DuplicateIndexError < StandardError; end

    STAR = "*"

    SPECIAL_GS_WORDS = %w(ora)

    def self.keywords(str)
      str.split(/\s+/).reject do |token|
        COMMON_TOKENS.include?(token)
      end
    end

    attr_reader :lookup

    def initialize()
      clear()
    end

    def clear()
      @lookup = {
        by_keyword: {},
        by_id: {},
        by_shop: {},
        by_item: {},
        by_town: {}
      }
    end

    def by_id(id)
      @lookup[:by_id].fetch(id, nil)
    end

    def has_id?(id)
      not by_id(id).nil?
    end

    def add_id(id, obj)
      id = id.to_s
      fail StandardError, "nil id" if id.nil?
      # shop id
      return if has_id?(id)
      if has_id?(id)
        _respond <<~ERROR
          DuplicateIdError:#{' '}
            new: \n#{PP.pp(obj, "")}\n
            old: \n#{PP.pp(by_id(id), "")}\n
        ERROR
      end
      @lookup[:by_id][id] = obj
    end

    def add_keyword(word, _id)
      # skip short words
      return if word.size < 4 and not (SPECIAL_GS_WORDS.include?(word) or word.is_i?)
      Log.out(word)
    end

    ##
    ## shop table storage
    ##
    def _sts(town:)
      all = Assets.glob("*.json")
      return all if town.eql?(STAR)
      all.select do |file| file.include?(town) end
    end

    def _load(town: STAR)
      _sts(town: town).each do |town_fs_data|
        town = Utils.read_json(town_fs_data)
        town.fetch(:shops, []).each do |shop|
          shop[:id] = shop[:town] + shop[:id]
          # numbers auto increment for each town
          # which means duplicate ids for each town
          add_id(shop[:id], shop)
          shop[:inv].each do |room|
            room[:items].each do |item|
              add_id(item[:id], item.merge(
                                  { branch: room[:branch],
                                    room_title: room[:room_title] }
                                ))
            end
          end
        end
      end
    end

    def query(**params)
    end
  end
end

module Bodega
  module SearchEngine
    ##
    ## resync raw assets from the CDN
    ##
    def self.sync()
      manifest = Utils.parse_json Bodega::Assets.get_remote("manifest.json")
      stale = manifest.fetch(:assets, []).select do |remote| Assets.is_stale?(remote) or Opts.flush end
      return if stale.empty?
      Utils.benchmark(template: ("sync".rjust(10) + " ... " + "completed".rjust(20) + " >> {{run_time}}"), label: :download) do
        stale.map do |remote|
          begin
            Thread.new do Assets.stream_download(remote) end
          rescue => exception
            Log.out(exception)
            Log.out(exception.backtrace)
          end
        end.map(&:value)
      end
    end
    ##
    ## the base index object
    ##
    @index ||= Index.new

    @index.clear if Opts["flush-index"]

    def self.build_index()
      Utils.benchmark(template: "built index in {{run_time}}") do
        @index._load()
      end
    end

    def self._index
      @index
    end

    def self.attach()
      SearchEngine.sync()
      SearchEngine.build_index() if Opts["force-index"]
    end
  end
end

module Bodega
  module Uploader
    # Upload endpoints - API method preferred for better reliability
    API_ENDPOINTS = [
      "https://funny-eclair-b53982.netlify.app/api/upload",
      # Fallback endpoints can be added here
    ]

    # Gist configuration (fallback only)
    GITHUB_REPO = "Nisugi/bodega"

    def self.upload_all_files(specific_files = nil)
      return if Opts["dry-run"]

      Log.out("Starting upload process...", label: :upload)

      # Try API upload first, fallback to gist/issue method
      if upload_via_api(specific_files)
        Log.out("Upload complete via API!", label: :upload)
        return
      else
        Log.out("API upload failed, falling back to gist/issue method...", label: :upload)
      end

      begin
        require 'net/http'
        require 'uri'
        require 'json'
        require 'digest'

        # Load upload tracking data
        upload_tracking = load_upload_tracking()

        # Get all JSON files or use specific files list
        files = specific_files || Assets.cached_files()

        if files.empty?
          Log.out("No files to upload", label: :upload)
          return
        end

        # Collect files that need uploading
        files_to_upload = {}
        skipped_files = []

        files.each do |file|
          basename = File.basename(file)
          if should_upload_file?(basename, upload_tracking)
            file_path = Assets.local_path(basename)
            if File.exist?(file_path)
              json_content = File.read(file_path)
              files_to_upload[basename] = json_content
              Log.out("Preparing #{basename} for upload", label: :upload)
            end
          else
            Log.out("Skipping #{basename} - already uploaded (same content)", label: :upload)
            skipped_files << basename
          end
        end

        # Upload all changed files in one gist
        if files_to_upload.any?
          Log.out("Creating gist with #{files_to_upload.size} file(s)...", label: :upload)

          gist_url = create_multi_file_gist(files_to_upload)

          if gist_url
            Log.out("Gist created: #{gist_url}", label: :upload)

            # Gist created successfully - no issue creation needed

            # Update tracking for all uploaded files
            files_to_upload.keys.each do |filename|
              update_upload_tracking(filename, upload_tracking)
            end
          end
        else
          Log.out("No files need uploading", label: :upload)
        end

        # Save updated tracking data
        save_upload_tracking(upload_tracking)

        Log.out("Upload complete! #{files_to_upload.size} uploaded, #{skipped_files.size} skipped", label: :upload)

      rescue => exception
        Log.out("Upload failed: #{exception.message}", label: :upload)
        Log.out(exception.backtrace.join("\n"), label: :upload)
      end
    end

    def self.create_multi_file_gist(files_hash)
      require 'net/http'
      require 'uri'
      require 'json'

      # GitHub Gists API endpoint
      api_url = "https://api.github.com/gists"

      timestamp = Time.now.strftime("%Y-%m-%d %H:%M:%S")

      # Create gist description
      town_names = files_hash.keys.map { |f| f.gsub('.json', '').gsub('_', ' ').split.map(&:capitalize).join(' ') }
      description = "Bodega shop data update - #{town_names.join(', ')} - #{timestamp}"

      # Build gist payload with all files
      gist_data = {
        description: description,
        public: true,
        files: {}
      }

      # Add each JSON file to the gist
      files_hash.each do |filename, content|
        begin
          # Validate that content is valid JSON before adding to gist
          JSON.parse(content)
          gist_data[:files][filename] = { content: content }
          Log.out("Added #{filename} to gist (#{content.length} bytes)", label: :upload)
        rescue JSON::ParserError => e
          Log.out("Skipping #{filename} - invalid JSON: #{e.message}", label: :upload)
        end
      end

      if gist_data[:files].empty?
        Log.out("No valid files to upload", label: :upload)
        return nil
      end

      Log.out("Gist payload size: #{gist_data.to_json.length} bytes", label: :upload)
      Log.out("Total files: #{gist_data[:files].size}", label: :upload)

      # Make API request
      uri = URI(api_url)
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['Accept'] = 'application/vnd.github.v3+json'
      # No authorization needed for public gists
      request['User-Agent'] = 'Bodega-Script/1.0'
      request.body = gist_data.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        return response_data['html_url']
      else
        Log.out("Gist creation failed with HTTP #{response.code}", label: :upload)
        Log.out("Response body: #{response.body}", label: :upload)
        error_msg = begin
          JSON.parse(response.body)['message']
        rescue
          "HTTP #{response.code}"
        end
        Log.out("Failed to create gist: #{error_msg}", label: :upload)
        return nil
      end
    end



    def self.split_large_file(filename, json_content)
      require 'json'

      begin
        data = JSON.parse(json_content)

        # Only split files that have a 'shops' array
        unless data['shops'] && data['shops'].is_a?(Array)
          Log.out("#{filename} doesn't have shops array, cannot split", label: :upload)
          return { filename => json_content }
        end

        shops = data['shops']
        total_shops = shops.length

        # Split into chunks of 50 shops each (should be ~2.5MB per chunk)
        chunk_size = 50
        chunks = {}

        shops.each_slice(chunk_size).with_index do |shop_chunk, index|
          chunk_number = index + 1
          total_chunks = (total_shops / chunk_size.to_f).ceil

          # Create chunk filename like: wehnimers_landing_part1of3.json
          base_name = filename.gsub('.json', '')
          chunk_filename = "#{base_name}_part#{chunk_number}of#{total_chunks}.json"

          # Create chunk data with same structure but subset of shops
          chunk_data = data.dup
          chunk_data['shops'] = shop_chunk
          chunk_data['chunk_info'] = {
            'original_file' => filename,
            'part' => chunk_number,
            'total_parts' => total_chunks,
            'shops_in_chunk' => shop_chunk.length,
            'total_shops' => total_shops
          }

          chunks[chunk_filename] = JSON.generate(chunk_data)

          Log.out("Created #{chunk_filename}: #{shop_chunk.length} shops, #{chunks[chunk_filename].bytesize} bytes", label: :upload)
        end

        return chunks

      rescue JSON::ParserError => e
        Log.out("Failed to parse #{filename} as JSON: #{e.message}", label: :upload)
        return { filename => json_content }
      end
    end

    def self.upload_via_api(specific_files = nil)
      begin
        require 'net/http'
        require 'uri'
        require 'json'

        # Get all JSON files or use specific files list
        files = specific_files || Assets.cached_files()

        if files.empty?
          Log.out("No files to upload", label: :upload)
          return true  # Nothing to upload is success
        end

        # Collect all files
        files_to_upload = {}
        files.each do |file|
          basename = File.basename(file)
          file_path = Assets.local_path(basename)
          if File.exist?(file_path)
            json_content = File.read(file_path)

            # Split large files (>5MB) into smaller chunks
            if json_content.bytesize > 5_000_000
              Log.out("#{basename} is large (#{json_content.bytesize} bytes), splitting...", label: :upload)
              split_files = split_large_file(basename, json_content)
              split_files.each do |split_name, split_content|
                files_to_upload[split_name] = split_content
                Log.out("Preparing #{split_name} for upload (#{split_content.bytesize} bytes)", label: :upload)
              end
            else
              files_to_upload[basename] = json_content
              Log.out("Preparing #{basename} for upload", label: :upload)
            end
          end
        end

        if files_to_upload.empty?
          Log.out("No valid files to upload", label: :upload)
          return true
        end

        # Try file-by-file upload to each endpoint
        API_ENDPOINTS.each do |endpoint|
          Log.out("Uploading #{files_to_upload.size} files to #{endpoint} one by one...", label: :upload)

          begin
            if upload_files_individually(endpoint, files_to_upload)
              Log.out("Successfully uploaded via API", label: :upload)
              return true
            end
          rescue => e
            Log.out("Endpoint #{endpoint} failed: #{e.message}", label: :upload)
          end
        end

        Log.out("All API endpoints failed", label: :upload)
        return false

      rescue => e
        Log.out("API upload error: #{e.message}", label: :upload)
        return false
      end
    end

    def self.trigger_workflow_with_gist(endpoint, gist_url, file_count)
      uri = URI(endpoint)

      # Send only the gist URL - tiny payload that Netlify can handle
      payload = {
        gist_url: gist_url,
        file_count: file_count,
        timestamp: Time.now.strftime("%Y-%m-%d %H:%M:%S UTC"),
        source: "bodega-script-gist"
      }

      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['User-Agent'] = 'Bodega-Script/2.0'
      request.body = payload.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        Log.out("API response: #{response_data['message']}", label: :upload)
        return true
      else
        Log.out("API returned HTTP #{response.code}: #{response.body}", label: :upload)
        return false
      end
    end

    def self.upload_to_endpoint(endpoint, files)
      uri = URI(endpoint)

      payload = {
        files: files,
        timestamp: Time.now.strftime("%Y-%m-%d %H:%M:%S UTC"),
        source: "bodega-script"
      }

      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['User-Agent'] = 'Bodega-Script/2.0'
      request.body = payload.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        Log.out("API response: #{response_data['message']}", label: :upload)
        return true
      else
        Log.out("API returned HTTP #{response.code}: #{response.body}", label: :upload)
        return false
      end
    end

    def self.upload_files_individually(endpoint, files)
      require 'net/http'
      require 'uri'
      require 'json'

      uri = URI(endpoint)
      session_id = Time.now.to_i.to_s + rand(1000).to_s
      total_files = files.size
      file_index = 0

      Log.out("Starting multi-file upload session: #{session_id}", label: :upload)

      files.each do |filename, content|
        file_index += 1
        is_final_file = (file_index == total_files)

        payload = {
          filename: filename,
          content: content,
          session_id: session_id,
          file_index: file_index,
          total_files: total_files,
          is_final: is_final_file,
          timestamp: Time.now.strftime("%Y-%m-%d %H:%M:%S UTC"),
          source: "bodega-script-individual"
        }

        json_data = payload.to_json
        Log.out("Uploading #{filename} (#{content.bytesize} bytes) - #{file_index}/#{total_files}", label: :upload)

        request = Net::HTTP::Post.new(uri)
        request['Content-Type'] = 'application/json'
        request['User-Agent'] = 'Bodega-Script/2.0'
        request.body = json_data

        response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true, read_timeout: 60) do |http|
          http.request(request)
        end

        if response.code.to_i >= 200 && response.code.to_i < 300
          response_data = JSON.parse(response.body)
          Log.out("#{filename} uploaded: #{response_data['message']}", label: :upload)

          # If this was the final file, check if gist was created
          if is_final_file && response_data['gist_url']
            Log.out("Multi-file upload complete! Gist: #{response_data['gist_url']}", label: :upload)
            return true
          end
        else
          Log.out("Failed to upload #{filename} - HTTP #{response.code}: #{response.body}", label: :upload)
          return false
        end

        # Small delay between uploads to be nice to the server
        sleep(0.5) unless is_final_file
      end

      return true
    rescue => e
      Log.out("Individual file upload error: #{e.message}", label: :upload)
      return false
    end

    def self.load_upload_tracking
      tracking_file = Assets.local_path("../upload_tracking.json")

      if File.exist?(tracking_file)
        begin
          JSON.parse(File.read(tracking_file))
        rescue JSON::ParserError
          Log.out("Invalid upload tracking file, starting fresh", label: :upload)
          {}
        end
      else
        {}
      end
    end

    def self.save_upload_tracking(tracking_data)
      tracking_file = Assets.local_path("../upload_tracking.json")
      File.write(tracking_file, JSON.pretty_generate(tracking_data))
    end

    def self.should_upload_file?(filename, tracking_data)
      file_path = Assets.local_path(filename)

      unless File.exist?(file_path)
        return false
      end

      # Calculate current file hash
      current_hash = Digest::SHA256.file(file_path).hexdigest

      # Check if we've uploaded this exact content before
      if tracking_data[filename] && tracking_data[filename]['content_hash'] == current_hash
        return false
      end

      true
    end

    def self.update_upload_tracking(filename, tracking_data)
      file_path = Assets.local_path(filename)

      tracking_data[filename] = {
        'content_hash' => Digest::SHA256.file(file_path).hexdigest,
        'last_uploaded' => Time.now.strftime("%Y-%m-%d %H:%M:%S"),
        'file_size' => File.size(file_path)
      }
    end

    def self.count_items_in_json(parsed_json)
      total_items = 0

      if parsed_json.is_a?(Hash) && parsed_json['shops']
        parsed_json['shops'].each do |shop|
          if shop['inv']
            shop['inv'].each do |room|
              if room['items']
                total_items += room['items'].length
              end
            end
          end
        end
      end

      total_items
    end
  end
end

module Bodega
  module CLI
    def self.help_menu()
      <<~HELP_MENU
          \n
        bodega.lic

          this script uses the new playershop system by Naos to parse in-game shop directories
          and generate JSON files that can be consumed by external systems.

          This script also exposes the Bodega module that other scripts may call.

        parse mode:
          --dry-run             run but print JSON to your FE                 [used primarily for testing]
          --town                index all shops in one town                   [used primarily for testing]
          --max-shop-depth      index only a certain number of shops per town [used primarily for testing]
          --max-item-depth      index only a certain number of items per shop [used primarily for testing]
          --shop                index a shop by name                          [used primarily for testing]
          --save                dump the results to the filesystem            [required in standalone mode]
          --out                 the location on the filesystem to write to    [defaults to $lich_dir/bodega/]
          --manifest            create a manifest file of the assets
          --upload              upload generated JSON files via API or gist fallback

        smart caching mode:
          --smart               enable smart caching - only inspect new/changed items (5-10x faster)
                                first run builds cache, subsequent runs use cache for massive speedup
                                automatically bootstraps from existing JSON files if available
          --force-full          force inspection of ALL items and rebuild cache (use with --smart)
                                useful for cache corruption or when you want fresh timestamps
          --cache-max-age=N     re-inspect cached items older than N days (default: 7 days)

        caching behavior:
          no flags              regular mode - inspect all items, ignore cache completely
          --smart               smart mode - use cache, inspect only new/changed items
          --smart --force-full  rebuild mode - inspect all items, update cache for future runs

        upload mode:
          --upload              upload existing JSON files from local filesystem

        search mode:
          --flush               forces a resync of the search index from the CDN
          --force-index         forces the search index to be built as fast as possible
          \n
      HELP_MENU
    end
    begin
      ##
      ## HALP
      ##
      if Opts.help
        respond CLI.help_menu()
        exit
      end
      ##
      ## handle Parser command
      ##
      if Opts.parser
        Log.out(Opts.to_h, label: :opts)
        created_files = Bodega::Parser.to_json()  if (Opts.save or Opts["dry-run"])
        Bodega::Parser.manifest() if Opts.manifest

        # Auto-upload after parsing if requested
        if Opts.upload and (Opts.save or Opts["dry-run"])
          # Only upload files that were just created during parsing
          Bodega::Uploader.upload_all_files(created_files)
        end
      end

      # Handle standalone upload mode
      if Opts.upload and not Opts.parser
        Log.out("Upload mode: uploading existing JSON files", label: :upload)
        Bodega::Uploader.upload_all_files
      end

      if Opts.search
        Bodega::SearchEngine.attach()
      end
    rescue => exception
      Log.out(exception)
    end
  end
end
