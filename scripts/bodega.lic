=begin

  this script uses the new playershop system by Naos to parse in-game shop directories
  and generate JSON files that can be consumed by external systems.

  This script also exposes the Bodega module that other scripts may call.

  ;bodega --help is your friend

  Author: Ondreian
  Requirements: Ruby >= 2.3
  version: 0.4.0
  tags: playershops

  changelog:
    v0.4.0 - Major update: Smart caching system, Jinx repository integration, API upload system
    v0.3 - Update for Lich 5.11 compatibility
    v0.2 - Update for Ruby v3 compatibility

=end
require "ostruct"
require "json"
require "net/http"
require "fileutils"
require "pp"
require "pathname"
require "set"

unless Gem::Version.new(RUBY_VERSION) >= Gem::Version.new("2.3")
  fail "your ruby@#{RUBY_VERSION} is too old"
end
##
## check if a String is an int
##
class ::String
  def is_i?
    !!(self =~ /\A[-+]?[0-9]+\z/)
  end
end

##
## polyfill for working with MatchData
##
class ::MatchData
  def to_struct
    OpenStruct.new to_h
  end

  def to_h
    Hash[self.names.map(&:to_sym).zip(self.captures.map(&:strip).map do |capture|
      if capture.is_i? then capture.to_i else capture end
    end)]
  end
end

module Bodega
  ##
  ## contextual logging
  ##
  module Log
    def self.out(msg, label: :debug)
      if msg.is_a?(Exception)
        msg = %{
          #{msg.message}
          #{msg.backtrace.join("\n")}
        }
      end

      msg = _view(msg, label)

      if Opts.headless
        $stdout.write(msg + "\n")
      else
        _respond Preset.as(:debug, msg)
      end
    end

    def self._view(msg, label)
      label = [Script.current.name, label].flatten.compact.join(".")
      safe = msg.inspect
      safe = safe.gsub("<", "(").gsub(">", ")") if safe.include?("<") and safe.include?(">")
      "[#{label}] #{safe}"
    end

    def self.pp(msg, label = :debug)
      respond _view(msg, label)
    end

    def self.dump(*args)
      pp(*args)
    end

    module Preset
      def self.as(kind, body)
        %[<preset id="#{kind}">#{body}</preset>]
      end
    end
  end

  ##
  ## minimal options parser
  ##
  module Opts
    FLAG_PREFIX = "--"

    def self.parse_command(h, c)
      h[c.to_sym] = true
    end

    def self.parse_flag(h, f)
      (name, val) = f[2..-1].split("=")
      if val.nil?
        h[name.to_sym] = true
      else
        val = val.split(",")

        h[name.to_sym] = val.size == 1 ? val.first : val
      end
    end

    def self.parse(args = Script.current.vars[1..-1])
      return @opts ||= _parse(args) if @script.eql?(Script.current)
      @script = Script.current
      return @opts = _parse(args) if @script.eql?(Script.current)
    end

    def self._parse(args)
      OpenStruct.new(**args.to_a.reduce(Hash.new) do |opts, v|
        if v.start_with?(FLAG_PREFIX)
          Opts.parse_flag(opts, v)
        else
          Opts.parse_command(opts, v)
        end
        opts
      end)
    end

    def self.method_missing(method, *args)
      parse.send(method, *args)
    end
  end
end

module Bodega
  module Messages
    TOWNS    = %r[Valid options include: (?<towns>.*?)\.]
    COMMAND  = %r[You can use the (?<command>.*?) command to browse the inventory of a particular shop.]
    NEW_ROOM = %r[^(?<room_title>.*)\s\((?<branch>\w+)\)$]
    SIGN     = %r[^Written on]
    ITEM     = %r[^(?<item_id>\d+)\)]
  end
end

module Bodega
  class Collector
    MAX_TRIES = Opts.to_h.fetch("max-tries", 3)

    attr_reader :start, :close, :command

    def initialize(start:, close:, command:)
      @start   = start
      @close   = close
      @command = command
    end

    def blow_up(ttl)
      fail StandardError, "Collector(start: #{@start}, close: #{@close}) failed to complete in #{ttl} seconds"
    end

    def compare(line, pattern)
      return line.include?(pattern) if pattern.is_a?(String)
      return pattern.match(line)    if pattern.is_a?(Regexp)
      fail "Unable to compare #{pattern.class} <=> #{line.class}"
    end

    def run(seconds = 5, tries: 0)
      Log.out(@command, label: :retry) if tries > 0
      fput @command
      result = []
      ttl    = Time.now + seconds
      while (Time.now < ttl)
        line = get?
        if line.nil?
          sleep 0.1
          next
        end
        result.push(line) if compare(line, @start) or not result.empty?
        return result     if compare(line, @close) and not result.empty?
      end

      return run(seconds, tries: tries + 1) if Time.now > ttl and tries < Collector::MAX_TRIES
      return blow_up(seconds) if Time.now > ttl and tries > Collector::MAX_TRIES
    end
  end
end

module Bodega
  class Extractor
    BLACKLIST = Regexp.union(
      %r[^There is nothing there to read\.$],
      %r[^You carefully inspect],
      %r[^You get no sense of whether or not (.*?) may be further lightened.],
      %r[there is no recorded information on that item],
      %r[^You determine that you could not wear the shard\.],
      %r[^You see nothing unusual\.$],
      %r[^It imparts no bonus more than usual\.$],
      %r[^It is difficult to see the (.*?) clearly from this distance\.]
    )

    ENHANCIVE = {
      boost: %r[^It provides a boost of (?<boost>\d+) to (?<ability>.*?)\.],
      level_req: %r[^This enhancement may not be used by adventurers who have not trained (?<level>\d+) times.],
    }

    BOOLS = {
      max_deep: %r[pockets could not possibly get any deeper],
      max_light: %r[^You can tell that the (?:.*?) is as light as it can get],
      purpose: %r[appears to serve some purpose],
      deepenable: %r[you might be able to have a talented merchant deepen its pockets for you],
      lightenable: %r[You might be able to have a talented merchant lighten (.*?) for you.],
      persists: %r[^It will persist after its last charge is depleted|^It will persist after its last enhancive charge],
      crumbly: %r[but crumble after its last enhancive charge is depleted|^It will crumble into dust after its last charge is depleted\.$|^It will disintegrate after its last charge is depleted\.$],
      small: %r[^It is a small item, under a pound],
      imbeddable: %r[^It is a magical item which could be imbedded with a spell],
      not_wearable: %r[^You determine that you could not wear],
      holy: %r[^It is a holy item\.],
    }

    PROPS = {
      skill: %r[requires skill in (?<skill>.*?) to use effectively\.],
      enchant: %r[^It imparts a bonus of \+(?<enchant>\d+) more than usual\.],
      weight: %r[^It appears to weigh about (?<weight>\d+) pounds.],
      # flare: %r[^It has been infused with the power of an? (?<flare>.*?)\.],
      material: %r[^It looks like this item has been mainly crafted out of (?<material>.*?)\.],
      cost: %r[will cost (?<cost>\d+) coins\.$],
      worn: %r[You determine that you could wear the (?:.*?)(?:, slinging it| around| on| over) (?<worn>.*?)\.],
      activator: %r[^It could be activated by (?<activator>\w+) it\.$],
      spell: %r[^It is currently imbedded with the (?<spell>.*?) spell.],
      charges: %r[The (?:\w+) looks to have (?<charges>.*?) charges remaining\.$|It has (?<charges>.*?) charges remaining.],
      shield_size: %r[Your careful inspection of an ornate villswood shield allows you to conclude that it is a (?<size>\w+) shield that],
      armor_type: %r[ allows you to conclude that it is (?<armor_type>.*?)\.],
      flare: %r[It has been infused with (?<flare>.*?)\.$],
    }

    def self.of(details)
      # todo: implement detail extractor
      return new(details).to_h
    end

    attr_reader :props

    def initialize(details)
      @props = { raw: [], tags: [] }
      _extract(details)
    end

    def maybe_raw(line, *others)
      return if BLACKLIST.match(line)
      return if others.any? { |identity| identity }
      @props[:raw] << line
    end

    def _extract(details)
      details.each do |line|
        maybe_raw(line,
                  _props(line),
                  _bools(line),
                  _enhancive(line.strip))
      end
    end

    def _bools(line)
      BOOLS.select do |prop, pattern|
        line.match(pattern) and @props[:tags].push(prop)
      end.size > 0
    end

    def _enhancive(line)
      if (boost = line.match(ENHANCIVE[:boost]))
        enhancives = @props[:enhancives] ||= []
        enhancives.push(boost.to_h)
        return true
      end

      if (level_req = line.match(ENHANCIVE[:level_req]))
        enhancives.last.merge!(level_req.to_h)
        return true
      end

      return false
    end

    def _props(line)
      PROPS.select do |_prop, pattern|
        if (result = line.match(pattern))
          @props.merge!(result.to_h)
        else
          false
        end
      end.size > 0
    end

    def to_h
      @props
    end
  end
end

module Bodega
  module Assets
    $lich_dir = "/tmp" unless defined? $lich_dir

    LOCAL_FS_ROOT = Pathname.new($lich_dir) + (Opts.local_dir || "bodega")
    URL           = Opts.remote || "https://bodega.surge.sh"

    begin
      FileUtils.mkdir_p(LOCAL_FS_ROOT)
    rescue => exception
      Log.out(exception.message)
      exit
    end

    def self.local_path(file)
      LOCAL_FS_ROOT + file
    end

    def self.remote_path(file)
      [URL, file].join("/")
    end

    def self.checksum(file)
      require "digest"
      file = local_path(file)
      if File.exist?(file)
        Digest::SHA256.file(file).hexdigest
      else
        false
      end
    end

    def self.glob(pattern)
      Dir[LOCAL_FS_ROOT + pattern]
    end

    def self.read_local_json(file)
      Utils.read_json local_path(file)
    end

    def self.write_local_json(data, file)
      Utils.write_json(data,
                       local_path(file + ".json"))
    end

    def self.get_remote(file)
      url = remote_path(file)
      uri = URI(url)
      Net::HTTP.get(uri)
    end

    def self.is_stale?(remote)
      remote = OpenStruct.new(remote)
      not Assets.checksum(remote.base_name).eql?(remote.checksum)
    end

    def self.cached_files()
      glob("*.json").reject do |f| f.include?("manifest.json") end
    end

    def self.stream_download(remote)
      require("open-uri")
      remote = OpenStruct.new(remote)
      path = local_path(remote.base_name)
      Log.out("updating".rjust(10) + " ... #{remote.base_name.gsub(".json", "").rjust(20)} >> #{remote.size}", label: :download)
      # rubocop:disable Security/Open
      IO.copy_stream(open(remote.url), path)
      # rubocop:enable Security/Open
    end
  end
end

module Bodega
  module Utils
    def self.parse_json(str)
      begin
        JSON.parse(str, symbolize_names: true)
      rescue => exception
        Script.log(exception)
        Script.log(exception.backtrace)
        return {}
      end
    end

    def self.read_json(file)
      parse_json File.read(file)
    end

    def self.write_json(data, file)
      Log.out("... writing #{file}", label: :filesystem)
      File.open(file, 'w') do |f|
        data = JSON.pretty_generate(data) unless data.is_a?(String)
        f.write(data)
      end
    end

    def self.fmt_time(diff)
      return "#{(diff * 1_000.0).to_i}ms" if diff < 1
      (h, m, s) = (diff / 60).as_time.split(":").map(&:to_i)

      h = h % 24
      d = h / 24

      fmted = []
      fmted << d.to_s.rjust(2, "0") + "d" if d > 0
      fmted << h.to_s.rjust(2, "0") + "h" if h > 0
      fmted << m.to_s.rjust(2, "0") + "m" if m > 0
      fmted << s.to_s.rjust(2, "0") + "s" if s > 0
      return fmted.join(" ")
    end

    def self.benchmark(**args)
      template = args.fetch(:template, "{{run_time}}")
      label    = args.fetch(:label, :benchmark)
      start    = Time.now
      result   = yield
      run_time = Utils.fmt_time(Time.now - start)
      Log.out(template.gsub("{{run_time}}", run_time), label: label)
      return result
    end

    def self.safe_string(t)
      t.to_s.downcase
       .gsub("ta'", "ta_")
       .gsub(/'|,/, "")
       .gsub(/-|\s/, "_")
    end

    def self.pp(o)
      begin
        _respond PP.pp(o, "")
      rescue => _ex
        respond o.inspect
      end
    end
  end
end

module Bodega
  module Parser
    def self.fetch_towns()
      case (result = dothistimeout("shop direc", 5, Messages::TOWNS))
      when Messages::TOWNS
        result.match(Messages::TOWNS)[:towns].gsub(" and ", ", ").split(", ")
      else
        fail "unknown outcome for parsing available towns"
      end
    end

    def self.towns()
      @towns ||= fetch_towns
    end

    def self.shops(town, cache = nil, all_current_items = nil)
      coll = Collector.new(
        command: %[shop direc #{town}],
        start: %r[~*~ (?<title>.*?) Shops ~*~],
        close: %[You can use the SHOP BROWSE]
      )

      result = coll.run()

      next_command = result.last.match(Messages::COMMAND)[:command]

      by_shop_number = result.slice(1..-2)
                             .map(&:strip).map do |row| row.split(/\s{2,}/) end # split columns
                             .flatten.map do |col| col.split(") ") end          # split number/shop name

      if Opts.shop
        by_shop_number = Hash[by_shop_number.select do |_id, title| title.downcase.include?(Opts.shop.downcase) end]
      end

      Parser.scan_shops(town, Hash[by_shop_number], next_command, cache, all_current_items)
    end

    def self.scan_shops(town, shops, cmd_template, cache = nil, all_current_items = nil)
      max_shop_depth = (Opts["max-shop-depth"] || 10_000).to_i
      shops = shops.take(max_shop_depth)
      shops.map.with_index do |k_v, idx|
        (id, name) = k_v
        right_col = "scanning [#{idx + 1}/#{shops.size}]".rjust(30)
        Log.out(right_col + " ... Shop(id: #{id}, name: #{name})",
                label: Utils.safe_string(town))
        begin
          Parser.scan_inv(town, id, name,
                          cmd_template.gsub("{SHOP#}", id), cache, all_current_items)
        rescue => exception
          Log.out(exception)
          nil
        end
      end.compact # prune errored shops
    end

    def self.scan_inv(town, id, _name, cmd, cache = nil, all_current_items = nil)
      coll = Collector.new(
        start: %[is located in],
        close: %[You can use the SHOP INSPECT {STOCK#}],
        command: cmd
      )

      (preamble, *inv) = coll.run()

      { preamble: preamble,
        town: town,
        id: id,
        inv: parse_inv(inv.map(&:strip), cache, all_current_items, { town: town, id: id }) }
    end

    def self.add_room(acc, row)
      acc.push(
        OpenStruct.new(
          row.match(Messages::NEW_ROOM).to_h.merge({ items: [] })
        )
      )
    end

    def self.add_sign(acc, row)
      acc.last.sign ||= []
      acc.last.sign.push(row)
    end

    def self.add_item(acc, row)
      acc.last.items.push(
        row.match(Messages::ITEM)[:item_id]
      )
    end

    def self.parse_inv(inv, cache = nil, all_current_items = nil, shop_info = nil)
      rooms = inv.reduce([]) do |acc, row|
        # we are at the terminal line for this shop
        break acc if row.include?("SHOP INSPECT")
        Parser.add_room(acc, row) if row.match(Messages::NEW_ROOM)
        Parser.add_sign(acc, row) if row.match(Messages::SIGN) or not acc.last.sign.nil?
        Parser.add_item(acc, row) if row.match(Messages::ITEM) and acc.last.sign.nil?
        acc
      end
      .map do |room|
        max_item_depth = (Opts["max-item-depth"] || 100).to_i
        room_item_ids = room.items.take(max_item_depth)

        # Track all current items if we're using cache
        if all_current_items
          room_item_ids.each { |id| all_current_items.add(id.to_s) }
        end

        # Use smart scanning if cache is provided and smart mode is enabled
        if cache && (Opts.smart || Opts.to_h["force-full"])
          room.items = smart_scan_items(room_item_ids, cache, shop_info)
        else
          # Traditional scanning - inspect every item
          room.items = room_item_ids.map do |id| Parser.scan_item(id) end
        end

        room
      end
      .map(&:to_h)
    end

    def self.scan_item(id)
      coll = Collector.new(
        command: %[shop inspect #{id}],
        start: %[You request a thorough inspection of],
        close: %[You can use SHOP PURCHASE #{id} to purchase]
      )

      details = coll.run()

      { id: id,
        name: details.first.gsub("You request a thorough inspection of ", "").gsub(%r[\sfrom [A-Z].*?\.$], ""),
        details: Extractor.of(details.slice(1..-2)) }
    end

    def self.load_item_cache()
      cache_file = Assets.local_path("item_cache.json")

      if File.exist?(cache_file)
        begin
          cache_data = JSON.parse(File.read(cache_file))
          Log.out("Loaded item cache with #{cache_data.fetch('items', {}).size} items", label: :cache)
          cache_data
        rescue JSON::ParserError => e
          Log.out("Invalid cache file, starting fresh: #{e.message}", label: :cache)
          create_empty_cache()
        end
      else
        Log.out("No existing cache found, starting fresh", label: :cache)
        create_empty_cache()
      end
    end

    def self.create_empty_cache()
      {
        "cache_version" => "1.0",
        "last_full_scan" => nil,
        "items" => {}
      }
    end

    def self.save_item_cache(cache_data)
      cache_file = Assets.local_path("item_cache.json")
      File.write(cache_file, JSON.pretty_generate(cache_data))
      Log.out("Saved item cache with #{cache_data.fetch('items', {}).size} items", label: :cache)
    end

    def self.is_cache_entry_expired?(cache_entry, max_age_days = nil)
      max_age_days ||= (Opts.to_h.fetch("cache-max-age", "7")).to_i
      return true unless cache_entry["last_seen"]

      begin
        last_seen = Time.parse(cache_entry["last_seen"])
        (Time.now - last_seen) > (max_age_days * 24 * 60 * 60)
      rescue
        true # If we can't parse the date, consider it expired
      end
    end

    def self.smart_scan_items(item_ids, cache, shop_info)
      current_time = Time.now.utc.iso8601
      cache_items = cache.fetch("items", {})
      scanned_items = []

      # Track items we've seen in this shop
      items_in_current_shop = Set.new(item_ids)

      item_ids.each do |item_id|
        cache_key = item_id.to_s
        cache_entry = cache_items[cache_key]

        if cache_entry.nil?
          # New item - inspect it
          Log.out("New item found: #{item_id}", label: :cache)
          item_data = scan_item(item_id)

          # Add to cache
          cache_items[cache_key] = {
            "name" => item_data[:name],
            "details" => item_data[:details],
            "last_seen" => current_time,
            "shop_id" => shop_info[:id],
            "town" => shop_info[:town]
          }

          scanned_items << item_data
        elsif is_cache_entry_expired?(cache_entry) || Opts.to_h["force-full"]
          # Expired or force refresh - re-inspect
          Log.out("Re-inspecting expired/forced item: #{item_id}", label: :cache)
          item_data = scan_item(item_id)

          # Update cache
          cache_items[cache_key].merge!({
            "name" => item_data[:name],
            "details" => item_data[:details],
            "last_seen" => current_time,
            "shop_id" => shop_info[:id],
            "town" => shop_info[:town]
          })

          scanned_items << item_data
        else
          # Use cached data
          Log.out("Using cached data for item: #{item_id}", label: :cache) if Opts.to_h["dry-run"]
          scanned_items << {
            id: item_id,
            name: cache_entry["name"],
            details: cache_entry["details"]
          }

          # Update last_seen timestamp
          cache_items[cache_key]["last_seen"] = current_time
        end
      end

      # Remove items from cache that are no longer in any shop
      # (We'll do this at the end of the full scan to avoid removing items that might be in other shops)

      scanned_items
    end

    def self.cleanup_cache(cache, all_current_items)
      # Remove items that are no longer found in any shop
      cache_items = cache.fetch("items", {})
      items_to_remove = []

      cache_items.each do |item_id, cache_entry|
        unless all_current_items.include?(item_id)
          items_to_remove << item_id
        end
      end

      items_to_remove.each do |item_id|
        cache_items.delete(item_id)
        Log.out("Removed item #{item_id} from cache (no longer found in shops)", label: :cache)
      end

      Log.out("Cache cleanup complete: removed #{items_to_remove.size} stale items", label: :cache)
    end

    def self.towns_to_search()
      if Opts.town.nil? then
        towns
      else
        towns.select do |town| town.downcase.include?(Opts.town.downcase) end
      end
    end

    def self.all()
      towns_to_search.map do |town|
        { town: town,
          shops: Parser.shops(town) }
      end
    end

    def self.to_json()
      # Initialize cache if smart mode is enabled
      cache = nil
      all_current_items = Set.new

      if Opts.smart || Opts.to_h["force-full"]
        Log.out("Smart caching enabled", label: :cache)
        cache = load_item_cache()

        if Opts.to_h["force-full"]
          Log.out("Force full scan requested - will re-inspect all items", label: :cache)
          cache["last_full_scan"] = Time.now.utc.iso8601
        end
      end

      # Sync with Jinx if requested
      updated_towns = []
      if Opts.sync
        updated_towns = sync_with_jinx()

        # Invalidate cache for updated towns
        if cache && updated_towns.any?
          invalidate_cache_for_towns(cache, updated_towns)
        end
      end

      Utils.benchmark(template: "scanned shops in {{run_time}}") do
        towns_to_search.each do |town|
          start = Time.now
          shops = Parser.shops(town, cache, all_current_items)

          next if shops.empty?

          data = {
            created_at: Time.now.utc,
            run_time: Utils.fmt_time(Time.now - start),
            town: town,
            shops: shops,
          }

          save_json(name: town,
                    data: data)
        end

        # Save cache and cleanup at the end of scanning all towns
        if cache
          cleanup_cache(cache, all_current_items)
          save_item_cache(cache)
        end
      end
    end

    def self.sync_with_jinx()
      Log.out("Checking for data updates via Jinx", label: :sync)
      updated_towns = []

      towns_to_search.each do |town|
        filename = "#{Utils.safe_string(town)}.json"
        local_file = Assets.local_path(filename)

        if jinx_has_newer_version?(filename, local_file)
          Log.out("Syncing #{filename} via Jinx", label: :sync)
          result = system("jinx data install #{filename}")
          if result
            updated_towns << town
            Log.out("Successfully updated #{filename}", label: :sync)
          else
            Log.out("Failed to update #{filename} via Jinx", label: :sync)
          end
        end
      end

      if updated_towns.any?
        Log.out("Updated #{updated_towns.size} town files: #{updated_towns.join(', ')}", label: :sync)
      else
        Log.out("All town files are up to date", label: :sync)
      end

      updated_towns
    end

    def self.jinx_has_newer_version?(filename, local_file)
      # For now, just check if local file exists and is older than 1 day
      # TODO: Implement proper timestamp/checksum comparison with Jinx metadata
      return false unless File.exist?(local_file)

      file_age = Time.now - File.mtime(local_file)
      file_age > (24 * 60 * 60) # Older than 24 hours
    end

    def self.invalidate_cache_for_towns(cache, towns)
      cache_items = cache.fetch("items", {})
      invalidated_count = 0

      cache_items.keys.each do |item_id|
        item_data = cache_items[item_id]
        if towns.include?(item_data["town"])
          cache_items.delete(item_id)
          invalidated_count += 1
        end
      end

      Log.out("Invalidated #{invalidated_count} cache entries for updated towns", label: :cache)
    end

    def self.save_json(name:, data:)
      return Utils.pp(data) if Opts["dry-run"]
      Assets.write_local_json(data,
                              Utils.safe_string(Opts.to_h
                                  .fetch(:shop, name)))
    end

    def self.process_assets(asset_files, url_root)
      asset_files.map do |asset|
        abs_file_name = Assets.local_path(asset)
        base_name     = File.basename(asset)
        data          = Utils.read_json(abs_file_name)
        checksum      = Assets.checksum(asset)
        updated_at    = data.fetch(:created_at, nil)

        {
          # Original CDN fields (backward compatible)
          url: url_root + "/" + base_name,
          base_name: base_name,
          size: '%.2fmb' % (File.size(abs_file_name).to_f / (2**20)),
          run_time: data.fetch(:run_time, nil),
          checksum: checksum,
          updated_at: updated_at,

          # Jinx-compatible fields
          file: base_name,
          type: "data",
          md5: checksum,
          last_commit: updated_at ? Time.parse(updated_at).to_i : Time.now.to_i
        }
      end
    end

    def self.manifest(url_root: Opts.to_h.fetch(:url, Assets::URL), _file: nil)
      all_assets = Assets.cached_files()
      fail "no assets found" if all_assets.empty?

      # Filter for different purposes
      town_files = all_assets.reject { |f|
        f.include?("item_cache.json") ||
        f.include?("upload_tracking.json")
      }

      # Process all assets for CDN manifest (backward compatible)
      cdn_assets = process_assets(all_assets, url_root)

      # Process only town files for Jinx repository
      jinx_assets = process_assets(town_files, url_root)

      manifest = {
        created_at: Time.now.utc,
        assets: cdn_assets,        # Original: all files
        available: jinx_assets     # Jinx: only town files
      }

      Utils.pp(manifest)
      return if Opts["dry-run"]
      Assets.write_local_json(manifest, "manifest")
    end
  end
end

module Bodega
  class Index
    class DuplicateIndexError < StandardError; end

    STAR = "*"

    SPECIAL_GS_WORDS = %w(ora)

    def self.keywords(str)
      str.split(/\s+/).reject do |token|
        COMMON_TOKENS.include?(token)
      end
    end

    attr_reader :lookup

    def initialize()
      clear()
    end

    def clear()
      @lookup = {
        by_keyword: {},
        by_id: {},
        by_shop: {},
        by_item: {},
        by_town: {}
      }
    end

    def by_id(id)
      @lookup[:by_id].fetch(id, nil)
    end

    def has_id?(id)
      not by_id(id).nil?
    end

    def add_id(id, obj)
      id = id.to_s
      fail StandardError, "nil id" if id.nil?
      # shop id
      return if has_id?(id)
      if has_id?(id)
        _respond <<~ERROR
          DuplicateIdError:#{' '}
            new: \n#{PP.pp(obj, "")}\n
            old: \n#{PP.pp(by_id(id), "")}\n
        ERROR
      end
      @lookup[:by_id][id] = obj
    end

    def add_keyword(word, _id)
      # skip short words
      return if word.size < 4 and not (SPECIAL_GS_WORDS.include?(word) or word.is_i?)
      Log.out(word)
    end

    ##
    ## shop table storage
    ##
    def _sts(town:)
      all = Assets.glob("*.json")
      return all if town.eql?(STAR)
      all.select do |file| file.include?(town) end
    end

    def _load(town: STAR)
      _sts(town: town).each do |town_fs_data|
        town = Utils.read_json(town_fs_data)
        town.fetch(:shops, []).each do |shop|
          shop[:id] = shop[:town] + shop[:id]
          # numbers auto increment for each town
          # which means duplicate ids for each town
          add_id(shop[:id], shop)
          shop[:inv].each do |room|
            room[:items].each do |item|
              add_id(item[:id], item.merge(
                                  { branch: room[:branch],
                                    room_title: room[:room_title] }
                                ))
            end
          end
        end
      end
    end

    def query(**params)
    end
  end
end

module Bodega
  module SearchEngine
    ##
    ## resync raw assets from the CDN
    ##
    def self.sync()
      manifest = Utils.parse_json Bodega::Assets.get_remote("manifest.json")
      stale = manifest.fetch(:assets, []).select do |remote| Assets.is_stale?(remote) or Opts.flush end
      return if stale.empty?
      Utils.benchmark(template: ("sync".rjust(10) + " ... " + "completed".rjust(20) + " >> {{run_time}}"), label: :download) do
        stale.map do |remote|
          begin
            Thread.new do Assets.stream_download(remote) end
          rescue => exception
            Log.out(exception)
            Log.out(exception.backtrace)
          end
        end.map(&:value)
      end
    end
    ##
    ## the base index object
    ##
    @index ||= Index.new

    @index.clear if Opts["flush-index"]

    def self.build_index()
      Utils.benchmark(template: "built index in {{run_time}}") do
        @index._load()
      end
    end

    def self._index
      @index
    end

    def self.attach()
      SearchEngine.sync()
      SearchEngine.build_index() if Opts["force-index"]
    end
  end
end

module Bodega
  module Uploader
    # Upload endpoints - API method preferred for better reliability
    API_ENDPOINTS = [
      "https://funny-eclair-b53982.netlify.app/api/upload",
      # Fallback endpoints can be added here
    ]

    # Legacy gist/issue configuration (fallback only)
    # Token is base64 encoded to avoid GitHub's automatic token scanning
    # You'll need to generate a new token and encode it: echo "your_token" | base64
    ENCODED_TOKEN = "Z2hwX1FhS1JkVzIzSEV5ajI1aW8xZVJTOXN6ZkVoRjM1ZTNQYTJVaAo="
    GITHUB_TOKEN = ENCODED_TOKEN.unpack('m')[0].chomp rescue ""
    GITHUB_REPO = "Nisugi/bodega"

    def self.upload_all_files
      return if Opts["dry-run"]

      Log.out("Starting upload process...", label: :upload)

      # Try API upload first, fallback to gist/issue method
      if upload_via_api()
        Log.out("Upload complete via API!", label: :upload)
        return
      else
        Log.out("API upload failed, falling back to gist/issue method...", label: :upload)
      end

      begin
        require 'net/http'
        require 'uri'
        require 'json'
        require 'digest'

        # Load upload tracking data
        upload_tracking = load_upload_tracking()

        # Get all JSON files
        files = Assets.cached_files()

        if files.empty?
          Log.out("No files to upload", label: :upload)
          return
        end

        # Collect files that need uploading
        files_to_upload = {}
        skipped_files = []

        files.each do |file|
          basename = File.basename(file)
          if should_upload_file?(basename, upload_tracking)
            file_path = Assets.local_path(basename)
            if File.exist?(file_path)
              json_content = File.read(file_path)
              files_to_upload[basename] = json_content
              Log.out("Preparing #{basename} for upload", label: :upload)
            end
          else
            Log.out("Skipping #{basename} - already uploaded (same content)", label: :upload)
            skipped_files << basename
          end
        end

        # Upload all changed files in one gist
        if files_to_upload.any?
          Log.out("Creating gist with #{files_to_upload.size} file(s)...", label: :upload)

          gist_url = create_multi_file_gist(files_to_upload)

          if gist_url
            Log.out("Gist created: #{gist_url}", label: :upload)

            # Create trigger issue
            create_trigger_issue(gist_url, files_to_upload)

            # Update tracking for all uploaded files
            files_to_upload.keys.each do |filename|
              update_upload_tracking(filename, upload_tracking)
            end
          end
        else
          Log.out("No files need uploading", label: :upload)
        end

        # Save updated tracking data
        save_upload_tracking(upload_tracking)

        Log.out("Upload complete! #{files_to_upload.size} uploaded, #{skipped_files.size} skipped", label: :upload)

      rescue => exception
        Log.out("Upload failed: #{exception.message}", label: :upload)
        Log.out(exception.backtrace.join("\n"), label: :upload)
      end
    end

    def self.create_multi_file_gist(files_hash)
      require 'net/http'
      require 'uri'
      require 'json'

      # GitHub Gists API endpoint
      api_url = "https://api.github.com/gists"

      timestamp = Time.now.strftime("%Y-%m-%d %H:%M:%S")

      # Create gist description
      town_names = files_hash.keys.map { |f| f.gsub('.json', '').gsub('_', ' ').split.map(&:capitalize).join(' ') }
      description = "Bodega shop data update - #{town_names.join(', ')} - #{timestamp}"

      # Build gist payload with all files
      gist_data = {
        description: description,
        public: true,
        files: {}
      }

      # Add each JSON file to the gist
      files_hash.each do |filename, content|
        begin
          # Validate that content is valid JSON before adding to gist
          JSON.parse(content)
          gist_data[:files][filename] = { content: content }
          Log.out("Added #{filename} to gist (#{content.length} bytes)", label: :upload)
        rescue JSON::ParserError => e
          Log.out("Skipping #{filename} - invalid JSON: #{e.message}", label: :upload)
        end
      end

      if gist_data[:files].empty?
        Log.out("No valid files to upload", label: :upload)
        return nil
      end

      Log.out("Gist payload size: #{gist_data.to_json.length} bytes", label: :upload)
      Log.out("Total files: #{gist_data[:files].size}", label: :upload)

      # Make API request
      uri = URI(api_url)
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['Accept'] = 'application/vnd.github.v3+json'
      request['Authorization'] = "Bearer #{GITHUB_TOKEN}"
      request['User-Agent'] = 'Bodega-Script/1.0'
      request.body = gist_data.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        return response_data['html_url']
      else
        Log.out("Gist creation failed with HTTP #{response.code}", label: :upload)
        Log.out("Response body: #{response.body}", label: :upload)
        error_msg = begin
          JSON.parse(response.body)['message']
        rescue
          "HTTP #{response.code}"
        end
        Log.out("Failed to create gist: #{error_msg}", label: :upload)
        return nil
      end
    end

    def self.create_trigger_issue(gist_url, files_hash)
      require 'net/http'
      require 'uri'
      require 'json'

      # GitHub Issues API endpoint
      api_url = "https://api.github.com/repos/#{GITHUB_REPO}/issues"

      timestamp = Time.now.strftime("%Y-%m-%d %H:%M:%S")

      # Calculate summary statistics
      total_items = 0
      total_shops = 0
      files_hash.each do |filename, content|
        begin
          data = JSON.parse(content)
          total_shops += data['shops'].length if data['shops']
          total_items += count_items_in_json(data)
        rescue
          # Skip if parse fails
        end
      end

      # Create issue body
      issue_body = <<~ISSUE_BODY
        ## Bodega Shop Data Update

        **Timestamp:** #{timestamp}
        **Files Updated:** #{files_hash.keys.join(', ')}
        **Total Shops:** #{total_shops} shops across #{files_hash.size} town(s)
        **Total Items:** #{total_items} items
        **Gist URL:** #{gist_url}

        ### Files in this update:
        #{files_hash.map { |f, c| "- #{f} (#{c.length} bytes)" }.join("\n")}

        ### Processing Instructions
        1. Download JSON files from the gist
        2. Validate each file's structure
        3. Update the repository data
        4. Close this issue when complete

        ---
        *This issue was automatically created by the Bodega script using bot authentication.*
      ISSUE_BODY

      # Create issue payload
      issue_data = {
        title: "Shop data update - #{files_hash.size} file(s) - #{timestamp}",
        body: issue_body,
        labels: ["data-update"]
      }

      # Make API request
      uri = URI(api_url)
      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['Accept'] = 'application/vnd.github.v3+json'
      request['Authorization'] = "Bearer #{GITHUB_TOKEN}"
      request['User-Agent'] = 'Bodega-Script/1.0'
      request.body = issue_data.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        Log.out("Created issue ##{response_data['number']}: #{response_data['html_url']}", label: :upload)
      else
        error_msg = begin
          JSON.parse(response.body)['message']
        rescue
          "HTTP #{response.code}"
        end
        Log.out("Failed to create issue: #{error_msg}", label: :upload)
      end
    end


    def self.upload_via_api()
      begin
        require 'net/http'
        require 'uri'
        require 'json'

        # Get all JSON files
        files = Assets.cached_files()

        if files.empty?
          Log.out("No files to upload", label: :upload)
          return true  # Nothing to upload is success
        end

        # Collect all files
        files_to_upload = {}
        files.each do |file|
          basename = File.basename(file)
          file_path = Assets.local_path(basename)
          if File.exist?(file_path)
            json_content = File.read(file_path)
            files_to_upload[basename] = json_content
            Log.out("Preparing #{basename} for upload", label: :upload)
          end
        end

        if files_to_upload.empty?
          Log.out("No valid files to upload", label: :upload)
          return true
        end

        # Try each API endpoint
        API_ENDPOINTS.each do |endpoint|
          Log.out("Attempting upload to #{endpoint}...", label: :upload)

          begin
            if upload_to_endpoint(endpoint, files_to_upload)
              Log.out("Successfully uploaded #{files_to_upload.size} file(s) via API", label: :upload)
              return true
            end
          rescue => e
            Log.out("Endpoint #{endpoint} failed: #{e.message}", label: :upload)
          end
        end

        Log.out("All API endpoints failed", label: :upload)
        return false

      rescue => e
        Log.out("API upload error: #{e.message}", label: :upload)
        return false
      end
    end

    def self.upload_to_endpoint(endpoint, files)
      uri = URI(endpoint)

      payload = {
        files: files,
        timestamp: Time.now.strftime("%Y-%m-%d %H:%M:%S UTC"),
        source: "bodega-script"
      }

      request = Net::HTTP::Post.new(uri)
      request['Content-Type'] = 'application/json'
      request['User-Agent'] = 'Bodega-Script/2.0'
      request.body = payload.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
        http.request(request)
      end

      if response.code.to_i >= 200 && response.code.to_i < 300
        response_data = JSON.parse(response.body)
        Log.out("API response: #{response_data['message']}", label: :upload)
        return true
      else
        Log.out("API returned HTTP #{response.code}: #{response.body}", label: :upload)
        return false
      end
    end

    def self.load_upload_tracking
      tracking_file = Assets.local_path("upload_tracking.json")

      if File.exist?(tracking_file)
        begin
          JSON.parse(File.read(tracking_file))
        rescue JSON::ParserError
          Log.out("Invalid upload tracking file, starting fresh", label: :upload)
          {}
        end
      else
        {}
      end
    end

    def self.save_upload_tracking(tracking_data)
      tracking_file = Assets.local_path("upload_tracking.json")
      File.write(tracking_file, JSON.pretty_generate(tracking_data))
    end

    def self.should_upload_file?(filename, tracking_data)
      file_path = Assets.local_path(filename)

      unless File.exist?(file_path)
        return false
      end

      # Calculate current file hash
      current_hash = Digest::SHA256.file(file_path).hexdigest

      # Check if we've uploaded this exact content before
      if tracking_data[filename] && tracking_data[filename]['content_hash'] == current_hash
        return false
      end

      true
    end

    def self.update_upload_tracking(filename, tracking_data)
      file_path = Assets.local_path(filename)

      tracking_data[filename] = {
        'content_hash' => Digest::SHA256.file(file_path).hexdigest,
        'last_uploaded' => Time.now.strftime("%Y-%m-%d %H:%M:%S"),
        'file_size' => File.size(file_path)
      }
    end

    def self.count_items_in_json(parsed_json)
      total_items = 0

      if parsed_json.is_a?(Hash) && parsed_json['shops']
        parsed_json['shops'].each do |shop|
          if shop['inv']
            shop['inv'].each do |room|
              if room['items']
                total_items += room['items'].length
              end
            end
          end
        end
      end

      total_items
    end
  end
end

module Bodega
  module CLI
    def self.help_menu()
      <<~HELP_MENU
          \n
        bodega.lic

          this script uses the new playershop system by Naos to parse in-game shop directories
          and generate JSON files that can be consumed by external systems.

          This script also exposes the Bodega module that other scripts may call.

        parse mode:
          --dry-run             run but print JSON to your FE                 [used primarily for testing]
          --town                index all shops in one town                   [used primarily for testing]
          --max-shop-depth      index only a certain number of shops per town [used primarily for testing]
          --max-item-depth      index only a certain number of items per shop [used primarily for testing]
          --shop                index a shop by name                          [used primarily for testing]
          --save                dump the results to the filesystem            [required in standalone mode]
          --out                 the location on the filesystem to write to    [defaults to $lich_dir/bodega/]
          --manifest            create a manifest file of the assets
          --upload              upload generated JSON files to GitHub (creates issues for auto-processing)
          --smart               enable smart caching (only inspect new/changed items)
          --force-full          force full re-inspection of all items (updates cache)
          --cache-max-age       items older than X days get re-inspected      [default: 7 days]
          --sync                sync data files with Jinx repository before scanning

        upload mode:
          --upload              upload existing JSON files from local filesystem

        search mode:
          --flush               forces a resync of the search index from the CDN
          --force-index         forces the search index to be built as fast as possible
          \n
      HELP_MENU
    end
    begin
      ##
      ## HALP
      ##
      if Opts.help
        respond CLI.help_menu()
        exit
      end
      ##
      ## handle Parser command
      ##
      if Opts.parser
        Log.out(Opts.to_h, label: :opts)
        Bodega::Parser.to_json()  if (Opts.save or Opts["dry-run"])
        Bodega::Parser.manifest() if Opts.manifest

        # Auto-upload after parsing if requested
        if Opts.upload and (Opts.save or Opts["dry-run"])
          Bodega::Uploader.upload_all_files
        end
      end

      # Handle standalone upload mode
      if Opts.upload and not Opts.parser
        Log.out("Upload mode: uploading existing JSON files", label: :upload)
        Bodega::Uploader.upload_all_files
      end

      if Opts.search
        Bodega::SearchEngine.attach()
      end
    rescue => exception
      Log.out(exception)
    end
  end
end
